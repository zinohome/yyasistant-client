#!/usr/bin/python3
# -*- coding:utf-8 -*-
"""
@author: ibmzhangjun@139.com
@file: app.py.py
@time: 2025/7/13 ä¸‹åˆ8:15
@desc: 
"""
import base64
import os
import io
import wave
import numpy as np
import audioop

from chainlit.input_widget import Switch
from config import config_from_dotenv
from utils.logger import logger
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
cfg = config_from_dotenv(os.path.join(BASE_DIR, '.env'), read_from_file=True)

PROFILE_NAME = "å¥åº·åŠ©æ‰‹"
PROFILE_ICON = "/public/profileimg/XiaoYan2.png"
PROFILE_DESC = "æˆ‘æ˜¯æ‚¨çš„å¥åº·åŠ©æ‰‹å°ç ”ï¼Œæˆ‘æ€»æ˜¯åœ¨è¿™é‡Œï¼Œéšæ—¶å‡†å¤‡å¸®åŠ©æ‚¨ã€‚æˆ‘èƒ½å¤Ÿä¸ºæ‚¨æä¾›å¤šæ–¹é¢ã€ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚"
PROFILE_STARTERS = [
    {
        "label": "å¥åº·ä¸å…»ç”Ÿ",
        "message": "ç»™æˆ‘ä»‹ç»ä¸€äº›å¥åº·å’Œå…»ç”Ÿæ–¹é¢çš„çŸ¥è¯†ã€‚",
        "icon": "/public/startterimg/chat.png"
    },
    {
        "label": "æ–‡è‰ºä¸æ–‡åŒ–ç”Ÿæ´»",
        "message": "é€€ä¼‘åå¦‚ä½•ä¸°å¯Œæ–‡è‰ºå’Œæ–‡åŒ–ç”Ÿæ´»ï¼Ÿ",
        "icon": "/public/startterimg/report.png"
    },
    {
        "label": "æ—¥å¸¸ç”Ÿæ´»åŠ©æ‰‹",
        "message": "åšæˆ‘çš„å°åŠ©æ‰‹ï¼Œé™ªæˆ‘èŠå¤©è§£é—·ã€‚",
        "icon": "/public/startterimg/knowledge.png"
    },
    {
        "label": "æ–°æŠ€èƒ½å­¦ä¹ ",
        "message": "é€€ä¼‘åæˆ‘å¯ä»¥å­¦å“ªäº›æ–°æŠ€èƒ½ï¼Œç»™æˆ‘æ¨èå‡ ä¸ªå§ã€‚",
        "icon": "/public/startterimg/doctor.png"
    },
]

# Define a threshold for detecting silence and a timeout for ending a turn
SILENCE_THRESHOLD = (
    3500  # Adjust based on your audio level (e.g., lower for quieter audio)
)
SILENCE_TIMEOUT = 1300.0  # Seconds of silence to consider the turn finished

import chainlit as cl
from utils.yyassistantapiclient import YYAssistantAPIClient


@cl.set_chat_profiles
async def chat_profile(current_user: cl.User):
    starters = [
        cl.Starter(label=s["label"], message=s["message"], icon=s["icon"])
        for s in PROFILE_STARTERS
    ]
    return [
        cl.ChatProfile(
            name=PROFILE_NAME,
            icon=PROFILE_ICON,
            markdown_description=PROFILE_DESC,
            starters=starters,
        )
    ]

@cl.on_chat_start
async def start():
    settings = await cl.ChatSettings(
        [
            Switch(id="auto_play_audio", label="è‡ªåŠ¨æ’­æ”¾è¯­éŸ³", default=True, description="å¼€å¯åä¼šè‡ªåŠ¨æ’­æŠ¥åŠ©æ‰‹çš„è¯­éŸ³å›å¤"),
        ]
    ).send()
    await settings_update(settings)
@cl.on_settings_update
async def settings_update(settings):
    cl.user_session.set("chat_settings", settings)
@cl.on_chat_start
async def start_chat():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯å’Œä¼šè¯
    base_url = cfg.SERVER_BASE_URL
    client = YYAssistantAPIClient(base_url)  # è¯·æ›¿æ¢ä¸ºå®é™…çš„APIåŸºç¡€URL
    # è·å–é»˜è®¤å¼•æ“
    default_engine = cfg.DEFAULT_AGENT_ENGINE
    if not default_engine:
        default_engine = await client.get_default_agent_engine()

    default_asr_engine = cfg.DEFAULT_ASR_ENGINE
    if not default_asr_engine:
        default_asr_engine = await client.get_default_asr_engine()

    default_tts_engine = cfg.DEFAULT_TTS_ENGINE
    if not default_tts_engine:
        default_tts_engine = await client.get_default_tts_engine()

    # åˆ›å»ºæ–°ä¼šè¯
    conversation = await client.create_agent_conversation(default_engine, {"input": "å¼€å§‹å¯¹è¯"})
    conversation_id = conversation.get("data", "") if isinstance(conversation, dict) else ""

    # å­˜å‚¨å®¢æˆ·ç«¯å’Œä¼šè¯IDåˆ°ç”¨æˆ·ä¼šè¯ä¸­
    cl.user_session.set("client", client)
    cl.user_session.set("conversation_id", conversation_id)
    cl.user_session.set("engine_name", default_engine)
    cl.user_session.set("asr_engine", default_asr_engine)
    cl.user_session.set("tts_engine", default_tts_engine)
    '''
    # å‘é€æ¬¢è¿æ¶ˆæ¯
    welcome_msg = f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„å¥åº·åŠ©æ‰‹**{PROFILE_NAME}** ğŸ˜Š\n\n"
    welcome_msg += f"{PROFILE_DESC}\n\n"
    welcome_msg += "æ‚¨å¯ä»¥ç›´æ¥è¾“å…¥é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ç‚¹å‡»å·¦ä¾§çš„å¿«æ·é—®é¢˜ä¸æˆ‘äº¤æµï½\n\n"
    welcome_msg += "**ä»¥ä¸‹æ˜¯ä¸€äº›æ‚¨å¯ä»¥å°è¯•çš„é—®é¢˜ï¼š**\n"
    for idx, s in enumerate(PROFILE_STARTERS, 1):
        welcome_msg += f"{idx}. **{s['label']}**ï¼š{s['message']}\n"

    await cl.Message(content=welcome_msg).send()
    '''

# ASRå¤„ç†â€”â€”å¤„ç†ä»@cl.on_audio_chunkæ¥çš„éŸ³é¢‘è½¬æ–‡å­—
async def asr_file_to_text(client, engine: str, audio_bytes: bytes, sample_rate=24000, sample_width=2):
    # ä¸å¤„ç†MP3ï¼Œç›´æ¥WAV PCMå¤„ç†
    data_b64 = base64.b64encode(audio_bytes[1]).decode("utf-8")  # ä½ æ¥å£asr_inference_wavçš„dataç›´æ¥ä¼ bytes
    #logger.info(f"asr_file_to_text data_b64: {data_b64}")
    asr_result = await client.asr_inference_wav(
        data=data_b64,
        user_id="chainlit-asr",
        engine=engine
    )
    # æŒ‰ä½ æ¥å£è¿”å›çš„jsonå¤„ç†
    if asr_result and isinstance(asr_result, dict) and "data" in asr_result:
        return asr_result["data"]
    elif asr_result and "text" in asr_result:
        return asr_result["text"]
    else:
        return ""


async def tts_text_to_speech(client, engine: str, text: str, max_len=150):
    # æŒ‰å­—ç¬¦åˆ‡ç‰‡ï¼Œä¸ä¼šæˆªæ–­ä¸­æ–‡
    text_list = [text[i:i + max_len] for i in range(0, len(text), max_len)]
    audio_bytes_segments = []

    for seg_text in text_list:
        tts_result = await client.tts_inference(
            data=seg_text,
            user_id="chainlit-tts",
            engine=engine
        )
        if tts_result and isinstance(tts_result, dict) and "data" in tts_result:
            audio_bytes = base64.b64decode(tts_result["data"])
            audio_bytes_segments.append(audio_bytes)

    return b"".join(audio_bytes_segments)

@cl.on_audio_start
async def on_audio_start():
    cl.user_session.set("audio_chunks", [])
    cl.user_session.set("silent_duration_ms", 0)
    cl.user_session.set("last_elapsed_time", 0)
    cl.user_session.set("is_speaking", False)
    return True


@cl.on_audio_chunk
async def on_audio_chunk(chunk: cl.InputAudioChunk):
    audio_chunks = cl.user_session.get("audio_chunks")
    if audio_chunks is None:
        audio_chunks = []
        cl.user_session.set("audio_chunks", audio_chunks)

    # è½¬ä¸ºnumpyæ–¹ä¾¿åç»­æ‹¼æ¥
    audio_chunk = np.frombuffer(chunk.data, dtype=np.int16)
    audio_chunks.append(audio_chunk)

    if chunk.isStart:
        cl.user_session.set("last_elapsed_time", chunk.elapsedTime)
        cl.user_session.set("is_speaking", True)
        return

    last_elapsed_time = cl.user_session.get("last_elapsed_time")
    if last_elapsed_time is None:
        last_elapsed_time = 0
    silent_duration_ms = cl.user_session.get("silent_duration_ms")
    if silent_duration_ms is None:
        silent_duration_ms = 0
    is_speaking = cl.user_session.get("is_speaking")
    cl.user_session.set("last_elapsed_time", chunk.elapsedTime)

    audio_energy = audioop.rms(chunk.data, 2)
    if audio_energy < SILENCE_THRESHOLD:
        silent_duration_ms += (chunk.elapsedTime - last_elapsed_time)
        cl.user_session.set("silent_duration_ms", silent_duration_ms)
        if silent_duration_ms > SILENCE_TIMEOUT and is_speaking:
            cl.user_session.set("is_speaking", False)
            await process_audio_input()
    else:
        # reset silence
        cl.user_session.set("silent_duration_ms", 0)
        if not is_speaking:
            cl.user_session.set("is_speaking", True)


async def process_audio_input():
    # åˆå¹¶æ‰€æœ‰audio_chunksï¼Œè½¬WAV
    audio_chunks = cl.user_session.get("audio_chunks")
    if not audio_chunks or not len(audio_chunks):
        cl.user_session.set("audio_chunks", [])
        return
    concatenated = np.concatenate(audio_chunks)
    wav_buffer = io.BytesIO()
    with wave.open(wav_buffer, "wb") as wav_file:
        wav_file.setnchannels(1)
        wav_file.setsampwidth(2)
        wav_file.setframerate(24000)
        wav_file.writeframes(concatenated.tobytes())
    wav_buffer.seek(0)
    '''
    frames = wav_file.getnframes()
    rate = wav_file.getframerate()
    duration = frames / float(rate)
    if duration <= 1.71:
        logger.debug("The audio is too short, please try again.")
        return
    '''
    audio_buffer = wav_buffer.getvalue()
    whisper_input = ("audio.wav", audio_buffer, "audio/wav")
    client: YYAssistantAPIClient = cl.user_session.get("client")
    asr_engine = cl.user_session.get("asr_engine")

    transcription = await asr_file_to_text(
        client,
        engine=asr_engine,
        audio_bytes=whisper_input
    )

    cl.user_session.set("audio_chunks", [])

    # å›æ˜¾åŸéŸ³é¢‘å’Œæ–‡æœ¬
    input_audio_el = cl.Audio(content=wav_buffer.getvalue(), mime="audio/wav")
    # é¡ºå¸¦æœªæ¥å¯¹é½ï¼Œå‘é€æ¶ˆæ¯æµ
    await cl.Message(
        author='æ‚¨',
        type="user_message",
        content=transcription,
        elements=[input_audio_el],
        metadata={"from_audio": True},
    ).send()

    # ç›´æ¥æŠŠæ–‡å­—â€œå‘â€ç»™on_messageé€»è¾‘
    await main(cl.Message(author='æ‚¨', content=transcription,metadata={"from_audio": True}))

@cl.on_message
async def main(message: cl.Message):
    # ä»ç”¨æˆ·ä¼šè¯ä¸­è·å–å®¢æˆ·ç«¯å’Œä¼šè¯ID
    client: YYAssistantAPIClient = cl.user_session.get("client")
    conversation_id = cl.user_session.get("conversation_id")
    engine_name = cl.user_session.get("engine_name")
    tts_engine = cl.user_session.get("tts_engine")
    asr_engine = cl.user_session.get("asr_engine")

    if not client or not conversation_id:
        await cl.Message(content="ä¼šè¯æœªåˆå§‹åŒ–ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•ã€‚", author="å¥åº·åŠ©æ‰‹").send()
        return

    # æå‰åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦æ¥æºäºè¯­éŸ³
    from_audio = False
    meta = getattr(message, "metadata", None)
    if meta and isinstance(meta, dict):
        from_audio = meta.get("from_audio", False)

    # è¯»å– ChatSettings è®¾ç½®
    chat_settings = cl.user_session.get("chat_settings") or {}
    auto_play_audio = chat_settings.get("auto_play_audio", True)  # é»˜è®¤ä¸ºTrue

    # è¾“å‡ºç©ºæ¶ˆæ¯ç”¨äºæµå¼tokenè¾“å‡º
    msg = cl.Message(content="", author="å¥åº·åŠ©æ‰‹")
    await msg.send()

    # è°ƒç”¨ Agent æ¨ç† APIè·å–æµå¼å“åº”
    try:
        full_response = ""
        # æ¥æ”¶æµå¼å“åº”
        # ------ æ–°å¢tryé”å®šgenerator -------
        stream_gen = client.agent_inference_stream(
            data=message.content,
            conversation_id=conversation_id,
            user_id="chainlit-user"
        )
        async for chunk in stream_gen:
            full_response += chunk
            await msg.stream_token(chunk)
        msg.content = full_response
        await msg.update()

        # åªåœ¨ from_audio æ—¶è¾¹ç”Ÿæˆè¾¹TTSï¼ŒåŒæ—¶åˆ†å¥æ›´è‡ªç„¶
        if from_audio and auto_play_audio and full_response.strip():
            tts_bytes = await tts_text_to_speech(client, tts_engine, full_response)
            if tts_bytes:
                audio_el = cl.Audio(content=tts_bytes, mime="audio/wav", auto_play=True)
                await cl.Message(content='', elements=[audio_el], author="å¥åº·åŠ©æ‰‹").send()

    except Exception as e:
        # åˆ›å»ºæ–°æ¶ˆæ¯æ˜¾ç¤ºé”™è¯¯ï¼Œè€Œä¸æ˜¯æ›´æ–°åŸæœ‰æ¶ˆæ¯
        await cl.Message(author="å¥åº·åŠ©æ‰‹", content=f"é”™è¯¯: {str(e)}").send()
        raise

if __name__ == "__main__":
    from chainlit.cli import run_chainlit
    run_chainlit(__file__)